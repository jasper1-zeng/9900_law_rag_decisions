"""
Service for building legal arguments.
"""
from typing import List, Dict, Any, Optional, Callable
from sqlalchemy.orm import Session
from app.api.schemas.arguments import BuildArgumentsRequest, BuildArgumentsResponse, RelatedCase
from app.db.conversation_repository import ConversationRepository
from rag.embeddings import generate_embeddings
from rag.retrieval import retrieve_with_reranking, retrieve_case_chunks_with_reranking
from rag.retrieval import retrieve_documents, retrieve_case_chunks
from rag.generation import (
    generate_insights, 
    generate_arguments, 
    generate_with_optimized_reasoning,
    generate_with_single_call_reasoning
)
from rag.llm_providers import get_llm_provider
import logging
import uuid
import re
import time

logger = logging.getLogger(__name__)

async def build_arguments_service(
    request: BuildArgumentsRequest,
    db: Optional[Session] = None,
    step_callback: Optional[Callable[[Dict[str, Any]], None]] = None,
    streaming_callback: Optional[Callable[[str], None]] = None
) -> BuildArgumentsResponse:
    """
    Build arguments based on case content using RAG components.
    
    Args:
        request: The arguments request containing case content and optional parameters
        db: Optional database session for conversation history
        step_callback: Optional callback for notifying about reasoning steps
        streaming_callback: Optional callback for streaming responses
        
    Returns:
        BuildArgumentsResponse: The response with related cases, insights and arguments
    """
    start_time = time.time()
    conversation_id = request.conversation_id or str(uuid.uuid4())
    logger.info(f"Building arguments for conversation: {conversation_id}")
    
    # Handle conversation
    conversation_repo = None
    
    # Initialize model_name at the top level so it's available in the exception handler
    model_name = "AI Assistant"
    selected_llm_model = request.llm_model
    
    # Standard disclaimer text (define once and reuse)
    disclaimer_text = lambda model: f"Analysis generated by {model}. For informational purposes only."
    
    if db:
        conversation_repo = ConversationRepository(db)
        
        # Use existing conversation if ID provided, otherwise create new one
        if conversation_id:
            # Verify conversation exists
            conversation = conversation_repo.get_conversation(conversation_id)
            if not conversation:
                # Create new conversation if ID doesn't exist
                conversation = conversation_repo.create_conversation(
                    title=f"Arguments: {request.case_title or request.case_content[:30]}...",
                    conversation_type="arguments",
                    metadata={
                        "case_title": request.case_title,
                        "case_topic": request.case_topic
                    }
                )
                conversation_id = conversation.id
        else:
            # Create new conversation
            conversation = conversation_repo.create_conversation(
                title=f"Arguments: {request.case_title or request.case_content[:30]}...",
                conversation_type="arguments",
                metadata={
                    "case_title": request.case_title,
                    "case_topic": request.case_topic
                }
            )
            conversation_id = conversation.id
        
        # Add user input to history
        conversation_repo.add_message(
            conversation_id=conversation_id,
            role="user",
            content=request.case_content
        )
    
    try:
        # Get the LLM provider to access model information
        llm_provider = get_llm_provider(for_chat=False, model=selected_llm_model)
        model_name = llm_provider.get_name()
        logger.info(f"Using LLM provider: {llm_provider.__class__.__name__} with model: {model_name}")
        
        # Generate embeddings for the content
        content_embedding = generate_embeddings(request.case_content)
        
        # Use a try-except block to handle reranker errors separately
        try:
            # Standardize retrieval limits for both documents and chunks
            retrieval_limit = 5  # Consistent limit for initial retrieval
            
            # Retrieve similar documents with reranking
            similar_docs = retrieve_with_reranking(
                content_embedding,
                request.case_content,  # Pass the original query text for reranking
                limit=retrieval_limit, 
                topic=request.case_topic,
                candidate_multiplier=3  # Retrieve 3x candidates for better reranking
            )
            
            # Also retrieve similar chunks with reranking using the same limit
            similar_chunks = retrieve_case_chunks_with_reranking(
                content_embedding,
                request.case_content,  # Pass the original query text for reranking
                limit=retrieval_limit,
                candidate_multiplier=3  # Use same multiplier for consistency
            )
        except Exception as reranker_error:
            # Log the error but continue with non-reranked retrieval
            logger.error(f"Reranking error: {reranker_error}")
            # Fall back to simple retrieval without reranking
            similar_docs = retrieve_documents(
                content_embedding,
                limit=retrieval_limit,
                topic=request.case_topic
            )
            similar_chunks = retrieve_case_chunks(
                content_embedding,
                limit=retrieval_limit
            )
        
        # Log similarity scores of retrieved items for debugging using direct print statements
        if similar_docs:
            print("\n==== DOCUMENT SIMILARITY SCORES ====")
            print(f"Top document similarity: {similar_docs[0].get('similarity', 0):.4f}")
            print(f"Document similarity range: {min([doc.get('similarity', 0) for doc in similar_docs]):.4f} - {max([doc.get('similarity', 0) for doc in similar_docs]):.4f}")
            print("All document scores: [")
            for doc in similar_docs:
                print(f"  ({doc.get('case_title', '')}, {doc.get('similarity', 0):.4f})")
            print("]")
        else:
            print("\n==== NO SIMILAR DOCUMENTS FOUND ====")
            
        if similar_chunks:
            print("\n==== CHUNK SIMILARITY SCORES ====")
            print(f"Top chunk similarity: {similar_chunks[0].get('similarity', 0):.4f}")
            print(f"Chunk similarity range: {min([chunk.get('similarity', 0) for chunk in similar_chunks]):.4f} - {max([chunk.get('similarity', 0) for chunk in similar_chunks]):.4f}")
            print("Sample chunk scores (first 3):")
            for i, chunk in enumerate(similar_chunks[:3]):
                print(f"  {i+1}. {chunk.get('case_title', '')}: {chunk.get('similarity', 0):.4f}")
        else:
            print("\n==== NO SIMILAR CHUNKS FOUND ====")
        
        # Apply minimum similarity threshold
        min_similarity = 0.4  # Reduced from 0.7 to allow more cases to be included
        similar_docs = [doc for doc in similar_docs if doc.get("similarity", 0) >= min_similarity]
        similar_chunks = [chunk for chunk in similar_chunks if chunk.get("similarity", 0) >= min_similarity]
        
        # Log after filtering using direct print
        print(f"\n==== AFTER FILTERING (threshold={min_similarity:.2f}) ====")
        print(f"Documents remaining: {len(similar_docs)}")
        print(f"Chunks remaining: {len(similar_chunks)}")
        
        # If no documents/chunks meet the threshold, lower the threshold further as a fallback
        if not similar_docs and not similar_chunks and (content_embedding is not None):
            fallback_min_similarity = 0.35  # lower threshold as fallback
            print(f"\n==== USING FALLBACK THRESHOLD {fallback_min_similarity:.2f} ====")
            
            # Try again with the original documents but lower threshold
            try:
                # Use raw retrieval without reranking to get more potentially relevant docs
                all_docs = retrieve_documents(
                    content_embedding,
                    limit=10,  # Get more documents to increase chance of finding something
                    topic=request.case_topic
                )
                all_chunks = retrieve_case_chunks(
                    content_embedding,
                    limit=10
                )
                
                # Show similarity scores for fallback documents
                if all_docs:
                    print("\n==== FALLBACK DOCUMENT SIMILARITY SCORES ====")
                    print(f"Top fallback document similarity: {all_docs[0].get('similarity', 0):.4f}")
                    print(f"Fallback document similarity range: {min([doc.get('similarity', 0) for doc in all_docs]):.4f} - {max([doc.get('similarity', 0) for doc in all_docs]):.4f}")
                    print("Fallback document scores: [")
                    for doc in all_docs:
                        print(f"  ({doc.get('case_title', '')}, {doc.get('similarity', 0):.4f})")
                    print("]")
                
                # Apply the lower threshold
                similar_docs = [doc for doc in all_docs if doc.get("similarity", 0) >= fallback_min_similarity]
                similar_chunks = [chunk for chunk in all_chunks if chunk.get("similarity", 0) >= fallback_min_similarity]
                
                print(f"\n==== AFTER FALLBACK FILTERING (threshold={fallback_min_similarity:.2f}) ====")
                print(f"Documents remaining: {len(similar_docs)}")
                print(f"Chunks remaining: {len(similar_chunks)}")
            except Exception as fallback_error:
                logger.error(f"Error in fallback retrieval: {fallback_error}")
        
        # Format the related cases from documents
        related_cases = [
            RelatedCase(
                title=doc['case_title'],
                url=doc.get("case_url", ""),  # Handle potential missing url
                summary=doc.get("reasons_summary", doc.get("catchwords", "")),  # Prefer reasons_summary, fallback to catchwords
                similarity_score=doc["similarity"],
                citation_number=doc.get("citation_number", "")
            )
            for doc in similar_docs
        ]
        
        # Track case IDs to avoid duplication
        case_ids_in_docs = {doc["id"] for doc in similar_docs}
        
        # Group chunks by case to avoid duplication
        case_id_to_chunks = {}
        for chunk in similar_chunks:
            case_id = chunk["case_id"]
            # Skip chunks from cases that are already included in documents
            if case_id in case_ids_in_docs:
                continue
                
            if case_id not in case_id_to_chunks:
                case_id_to_chunks[case_id] = []
            case_id_to_chunks[case_id].append(chunk)
        
        # Add unique cases from chunks if not already in related_cases
        existing_urls = {case.url for case in related_cases}
        for case_id, chunks in case_id_to_chunks.items():
            # Get first chunk for case metadata
            chunk = chunks[0]
            if chunk.get("case_url") and chunk["case_url"] not in existing_urls:
                related_cases.append(
                    RelatedCase(
                        title=chunk['case_title'],
                        url=chunk.get("case_url", ""),
                        summary=f"Relevant section: {chunk['chunk_text']}",  # Use full chunk text
                        similarity_score=chunk["similarity"],
                        citation_number=chunk.get("citation_number", "")
                    )
                )
                existing_urls.add(chunk["case_url"])
        
        # Sort related cases by similarity score
        related_cases.sort(key=lambda x: x.similarity_score, reverse=True)
        
        # Prepare combined context for generation
        combined_context = []
        
        # Add document context using full reasons instead of summaries
        for doc in similar_docs:
            combined_context.append({
                "type": "document",
                "id": doc["id"],
                "case_title": doc["case_title"],
                "reasons_summary": doc.get("reasons", doc.get("reasons_summary", "")),  # Prefer full reasons
                "citation_number": doc.get("citation_number", ""),
                "case_url": doc.get("case_url", ""),
                "similarity": doc["similarity"]
            })
        
        # Add chunk context from unique cases only (those not in documents)
        for case_id, chunks in case_id_to_chunks.items():
            # Get the most relevant chunk for this case
            best_chunk = max(chunks, key=lambda x: x.get("similarity", 0))
            combined_context.append({
                "type": "chunk",
                "chunk_id": best_chunk["chunk_id"],
                "case_id": best_chunk["case_id"],
                "case_title": best_chunk["case_title"],
                "reasons_summary": best_chunk.get("reasons", best_chunk["chunk_text"]),  # Prefer full reasons if available
                "citation_number": best_chunk.get("citation_number", ""),
                "case_url": best_chunk.get("case_url", ""),
                "similarity": best_chunk["similarity"]
            })
        
        # Sort combined context by similarity score
        combined_context.sort(key=lambda x: x["similarity"], reverse=True)
        
        # Limit to top 4 most relevant items for final context
        combined_context = combined_context[:4]
        
        # Print the final context that will be used for generation
        print("\n==== FINAL CONTEXT FOR GENERATION ====")
        for i, ctx in enumerate(combined_context):
            print(f"{i+1}. {ctx.get('case_title', 'Unknown')} ({ctx.get('type', 'unknown')}) - Similarity: {ctx.get('similarity', 0):.4f}")
            text = ctx.get('reasons_summary', '')
            print(f"   Content: {text[:150]}..." if len(text) > 150 else f"   Content: {text}")
        
        # Record the retrieved documents for history
        retrieved_docs_for_history = [
            {
                "title": doc.get("case_title", "Unknown"),
                "content": doc.get("reasons_summary", "")[:300] + "...",  # Still limit for history UI
                "similarity": doc.get("similarity", 0),
                "citation_number": doc.get("citation_number", "")
            }
            for doc in combined_context  # Use the optimized context
        ]
        
        # Generate arguments with step-by-step reasoning
        # Define a list to collect the reasoning steps for storing in conversation history
        reasoning_steps = []
        
        # Record the callback function that will also store steps
        def record_step(step: Dict[str, Any]):
            # Print each step as it's received
            step_name = step.get("step", "Unknown Step")
            print(f"\n--- RECEIVED STEP: {step_name} ---")
            output = step.get("output", "")[:200]  # Only print first 200 chars
            print(f"Output preview: {output}...")
            
            # Append to list and call original callback
            reasoning_steps.append(step)
            if step_callback:
                step_callback(step)
        
        # Choose between multi-step and single-call approach based on request parameter
        print(f"Using {'single-call' if request.use_single_call else 'multi-step'} reasoning approach")
        
        if request.use_single_call:
            # Use the single-call approach
            result = generate_with_single_call_reasoning(
                request.case_content,
                combined_context,
                topic=request.case_topic,
                llm_model=selected_llm_model
            )
            
            # For compatibility with step-based storage
            if 'token_usage' in result and db and conversation_repo:
                # Create a simple step structure for single-call approach
                reasoning_steps = [{
                    "step": "Single-Call Generation",
                    "instructions": "Generate legal arguments with internal step-by-step reasoning",
                    "output": result.get("final_output", ""),
                    "metrics": {
                        "input_tokens": result.get("token_usage", {}).get("input_tokens", 0),
                        "output_tokens": result.get("token_usage", {}).get("output_tokens", 0),
                        "execution_time_seconds": result.get("execution_time", 0)
                    }
                }]
        else:
            # Use the original multi-step approach
            result = generate_with_optimized_reasoning(
                request.case_content,
                combined_context,
                topic=request.case_topic,
                step_callback=record_step,
                llm_model=selected_llm_model  # Pass the selected model
            )
        
        # Print final output without truncation
        print("\n==== FINAL RAW OUTPUT ====")
        try:
            if "final_output" in result:
                output = result["final_output"]
                print(output)
            elif "output" in result:
                output = result["output"]
                print(output)
                result["final_output"] = output
            else:
                print("No final output found in result")
                steps = result.get("reasoning_steps", result.get("steps", []))
                if steps:
                    last_step = steps[-1]
                    output = last_step.get("output", "")
                    print("Using last step output as final output:")
                    print(output)
                    result["final_output"] = output
        except Exception as output_error:
            print(f"Error printing final output: {output_error}")
            result["final_output"] = "Error generating final output."
        
        # Add disclaimer to response
        disclaimer = f"DISCLAIMER: Analysis generated by {model_name}. For informational purposes only."
        
        # Limit the number of related cases shown to match context limit
        if len(related_cases) > 4:
            print(f"\n==== LIMITING DISPLAYED CASES ====")
            print(f"Reducing displayed cases from {len(related_cases)} to 3")
            related_cases = related_cases[:3]
            
        # Show final cases being returned
        print("\n==== FINAL RELATED CASES RETURNED TO FRONTEND ====")
        for i, case in enumerate(related_cases):
            print(f"{i+1}. {case.title}{case.citation_number and f' ({case.citation_number})' or ''} - Similarity: {case.similarity_score:.4f}")
            summary = case.summary
            print(f"   Summary: {summary}")
        
        # Store the result in the conversation history if DB available
        if db and conversation_repo:
            # Format the detailed content for storage in conversation history
            detailed_content = ""
            
            # Only include one disclaimer in the stored content
            actual_model = model_name
            detailed_content += f"## DISCLAIMER\n{disclaimer_text(actual_model)}\n\n"
            
            # Simply append the full raw LLM output
            detailed_content += result.get("final_output", "Error: No output generated.")
            
            # Add related cases if available
            if related_cases and len(related_cases) > 0:
                detailed_content += "\n\n## Related Cases\n"
                for case in related_cases:
                    detailed_content += f"### [{case.title}{case.citation_number and f' ({case.citation_number})' or ''}]({case.url})\n"
                    detailed_content += f"{case.summary}\n"
                    detailed_content += f"**Similarity**: {(case.similarity_score * 100):.1f}%\n\n"
            
            # Store the assistant's response with the detailed content
            try:
                conversation_repo.add_message(
                    conversation_id=conversation_id,
                    role="assistant",
                    content=detailed_content,  # Store the full formatted content
                    reasoning_steps=reasoning_steps,
                    retrieved_documents=retrieved_docs_for_history
                )
            except Exception as db_error:
                logger.error(f"Error storing conversation: {db_error}")
        
        return BuildArgumentsResponse(
            disclaimer=disclaimer,
            related_cases=related_cases,
            # Simply pass the raw LLM output to the frontend
            raw_content=result.get("final_output", "Error: No output generated."),
            conversation_id=conversation_id
        )
    
    except Exception as e:
        error_message = f"Error building arguments: {str(e)}"
        logger.error(error_message)
        
        # Store error in conversation history if DB available
        if db and conversation_repo and conversation_id:
            conversation_repo.add_message(
                conversation_id=conversation_id,
                role="assistant",
                content=f"Error: {error_message}"
            )
        
        # Return a minimal response with error message using same model_name from initialization
        return BuildArgumentsResponse(
            disclaimer=f"DISCLAIMER: Analysis generated by {model_name}. For informational purposes only.",
            related_cases=[],
            raw_content="Error: Unable to generate arguments due to a system error",
            conversation_id=conversation_id  # Return conversation ID even on error
        ) 