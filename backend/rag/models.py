"""
RAG Models Module

This module handles the loading and management of models used in the RAG system.
This is separate from the llm_providers.py module which handles the LLM integration.
"""
from typing import Dict, Any, Optional
from app.config import settings
import logging

logger = logging.getLogger(__name__)

# Dictionary to store loaded models
_models = {}

def load_llm_model(model_name: str = "gpt-3.5-turbo") -> Any:
    """
    Load and cache the LLM model.
    
    Args:
        model_name: The name of the LLM model to load
        
    Returns:
        Any: The loaded model
    """
    # Check if model is already loaded
    if f"llm_{model_name}" in _models:
        return _models[f"llm_{model_name}"]
    
    # Use the LLM provider system instead
    try:
        from rag.llm_providers import get_llm_provider
        model = get_llm_provider(model=model_name)
        _models[f"llm_{model_name}"] = model
        return model
    except ImportError:
        logger.warning("LLM provider system not available, using dummy model")
        
        # Fallback to dummy model
        class DummyLLMModel:
            def __init__(self, name):
                self.name = name
            
            def generate(self, prompt, **kwargs):
                return f"This is a response generated by {self.name} for: {prompt[:50]}..."
        
        model = DummyLLMModel(model_name)
        _models[f"llm_{model_name}"] = model
        
        return model

def get_model(model_type: str, model_name: Optional[str] = None) -> Any:
    """
    Get a model of the specified type.
    
    Args:
        model_type: The type of model (currently only 'llm' is supported here)
        model_name: The name of the model
        
    Returns:
        Any: The requested model
    """
    if model_type == "llm":
        return load_llm_model(model_name)
    else:
        raise ValueError(f"Unknown model type: {model_type}. For embedding models, use rag.embeddings.get_model() directly.")

def init_models():
    """
    Initialize LLM models at startup.
    """
    # Load LLM model (will use the LLM provider system now)
    load_llm_model(settings.CHAT_LLM_MODEL) 